{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data set for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.abspath(os.path.join('.'))\n",
    "module_path = os.path.abspath(os.path.join('..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prabhat_tripathi/dev/Project/NLP/models\n"
     ]
    }
   ],
   "source": [
    "print(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIC_NLP_LIB_HOME=current_path + \"/indic_nlp_lib\"\n",
    "INDIC_NLP_RESOURCES=current_path + \"/indic_nlp_resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'{}/src'.format(INDIC_NLP_LIB_HOME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/prabhat_tripathi/dev/Project/NLP/models/model4/indic_nlp_lib/src', '', '/anaconda3/lib/python36.zip', '/anaconda3/lib/python3.6', '/anaconda3/lib/python3.6/lib-dynload', '/anaconda3/lib/python3.6/site-packages', '/anaconda3/lib/python3.6/site-packages/aeosa', '/anaconda3/lib/python3.6/site-packages/IPython/extensions', '/Users/prabhat_tripathi/.ipython', '/Users/prabhat_tripathi/dev/Project/NLP/models']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_prep;\n",
    "import utils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prabhat_tripathi/dev/Project/NLP/models/model4/indic_nlp_lib/src/indicnlp/script/indic_scripts.py:116: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ALL_PHONETIC_VECTORS= ALL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/Users/prabhat_tripathi/dev/Project/NLP/models/model4/indic_nlp_lib/src/indicnlp/script/indic_scripts.py:117: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  TAMIL_PHONETIC_VECTORS=TAMIL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/Users/prabhat_tripathi/dev/Project/NLP/models/model4/indic_nlp_lib/src/indicnlp/script/english_script.py:113: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ENGLISH_PHONETIC_VECTORS=ENGLISH_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n"
     ]
    }
   ],
   "source": [
    "from indicnlp import loader\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "remove_nuktas=True\n",
    "factory=IndicNormalizerFactory()\n",
    "normalizer=factory.get_normalizer(\"hi\",remove_nuktas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: क़ क़  output (clean): क क\n"
     ]
    }
   ],
   "source": [
    "input_text=u\"\\u0958 \\u0915\\u093c\"\n",
    "output_text=normalizer.normalize(input_text)\n",
    "print(\"input:\", input_text, \" output (clean):\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en, data_hi = data_prep.loadParallelCorpus('../../data/parallel/IITB.en-hi.en', '../../data/parallel/IITB.en-hi.hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1561841, 1561841)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_rows=data_en.split(\"\\n\")\n",
    "hi_rows=data_hi.split(\"\\n\")\n",
    "(len(en_rows),len(hi_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12565  sentences bad. Removing them...\n"
     ]
    }
   ],
   "source": [
    "# 1. remove sentenses with more than half non-alpha numeric characters\n",
    "to_remove = [i for i, val in enumerate(en_rows) if data_prep.isNonAlpha(val) == True]\n",
    "print(len(to_remove), \" sentences bad. Removing them...\")\n",
    "\n",
    "# process in reverse to avoid recomputing offsets\n",
    "for index in reversed(to_remove):\n",
    "    del en_rows[index]\n",
    "    del hi_rows[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normalize hindi Nukta characters\n",
    "hi_rows = [normalizer.normalize(x) for x in hi_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Shuffle corpora. The sentences in the corpora are not mixed up in the original order\n",
    "en_rows_all, hi_rows_all = data_prep.shuffle(en_rows, hi_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(en_sentences, hi_sentences):\n",
    "    en_sentences = [ line.lower() for line in en_sentences]\n",
    "    \n",
    "    en_vocab_dict = data_prep.buildEngVocab(en_sentences)\n",
    "    hi_vocab_dict = data_prep.buildHinVocab(hi_sentences)\n",
    "\n",
    "    en_vocab = list(map(lambda x: x[0], sorted(en_vocab_dict.items(), key=lambda x: -x[1])))\n",
    "    hi_vocab = list(map(lambda x: x[0], sorted(hi_vocab_dict.items(), key=lambda x: -x[1])))\n",
    "    \n",
    "    # Zipf's law\n",
    "    # https://openreview.net/pdf?id=Bk8N0RLxx - limit vocab to 50k\n",
    "    if (len(en_vocab) > 50000):\n",
    "      en_vocab = en_vocab[:50000]\n",
    "    if (len(hi_vocab) > 50000):\n",
    "      hi_vocab = hi_vocab[:50000]\n",
    "\n",
    "    # Build a Word to Index Dictionary for English\n",
    "    start_idx = 4\n",
    "    en_word2idx = dict([(word, idx + start_idx) for idx, word in enumerate(en_vocab)])\n",
    "    en_word2idx['<pad>'] = 0  # Padding\n",
    "    en_word2idx['<start>'] = 1\n",
    "    en_word2idx['<end>'] = 2  # End of sentence\n",
    "    en_word2idx['<ukn>'] = 3  # Unknown words\n",
    "   \n",
    "\n",
    "    en_vocab.append('<ukn>');\n",
    "    en_vocab.append('<start>');\n",
    "    en_vocab.append('<end>');\n",
    "    en_vocab.append('<pad>');\n",
    "    \n",
    "    # Build reverse Index to Word Dictionary for English using the already created Word to Index Dictionary\n",
    "    en_idx2word = dict([(idx, word) for word, idx in en_word2idx.items()])\n",
    "\n",
    "    # Build a Word to Index Dictionary for Hindi\n",
    "    start_idx = 4\n",
    "    hi_word2idx = dict([(word, idx + start_idx) for idx, word in enumerate(hi_vocab)])\n",
    "    hi_word2idx['<pad>'] = 0  # Padding\n",
    "    hi_word2idx['<start>'] = 1\n",
    "    hi_word2idx['<end>'] = 2  # End of sentence\n",
    "    hi_word2idx['<ukn>'] = 3  # Unknown\n",
    "   \n",
    "\n",
    "    hi_vocab.append('<ukn>');\n",
    "    hi_vocab.append('<start>');\n",
    "    hi_vocab.append('<end>');\n",
    "    hi_vocab.append('<pad>');\n",
    "\n",
    "    \n",
    "    # Build the inverse Index to Word Dictionary for Hindi using the already created Word to Index Dictionary\n",
    "    hi_idx2word = dict([(idx, word) for word, idx in hi_word2idx.items()])\n",
    "\n",
    "    # Encode words in senteces by their index in Vocabulary\n",
    "    x = [[en_word2idx.get(word.strip(',.\" ;:)(][?!'), 3) for word in sentence.split()] for sentence in en_sentences]\n",
    "    y = [[hi_word2idx.get(word.strip(',.\" ;:)(।|][?!'), 3) for word in sentence.split()] for sentence in hi_sentences]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2\n",
    "        if abs(n1 - n2) < 0.3 * n:\n",
    "            if n1 <= max_sent_length and n2 <= max_sent_length:\n",
    "              # ignore single word sentences with only unknown word\n",
    "              if (n1 > 1 or (x[i][0] != 0 and x[i][0] != en_word2idx[''])) and (n2 > 1 or (y[i][0] != 0 and y[i][0] != hi_word2idx[''])):\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "\n",
    "    return X, Y, en_word2idx, en_idx2word, en_vocab, hi_word2idx, hi_idx2word, hi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_location = \"out/parallel_trainv4.p\"\n",
    "utils.save_pickle(dataset_save_location, prepare_dataset(en_rows_all, hi_rows_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696695, 696695, 50004, 50004)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us load and check for sanity\n",
    "X_all, Y_all, en_word2idx_all, en_idx2word_all, en_vocab_all, hi_word2idx_all, hi_idx2word_all, hi_vocab_all = utils.load_pickle_dataset(dataset_save_location)\n",
    "len(X_all), len(Y_all), len(en_vocab_all), len(hi_vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but this desire is not all bad \n",
      "\n",
      "किन्तु यह अभिलाषा पूर्णतः बुरी नहीं है \n",
      "\n",
      "see this also \n",
      "\n",
      "यह भी देखें \n",
      "\n",
      "nitrogen constitutes nearly four - fifths of the air by volume \n",
      "\n",
      "वायु में नाइट्रोजन या <ukn> की मात्रा लगभग 4/5 भाग होती है \n",
      "\n",
      "memento \n",
      "\n",
      "<ukn> \n",
      "\n",
      "acoustics \n",
      "\n",
      "<ukn> \n",
      "\n",
      "substance \n",
      "\n",
      "अर्थ \n",
      "\n",
      "illiterate \n",
      "\n",
      "अनपढ \n",
      "\n",
      "<ukn> \n",
      "\n",
      "स्वच्छंद \n",
      "\n",
      "<ukn> irani \n",
      "\n",
      "बोमन ईरानी \n",
      "\n",
      "eggplant \n",
      "\n",
      "<ukn> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    data_prep.printSentence(X_all[n], en_idx2word_all)\n",
    "    print(\"\\n\")\n",
    "    data_prep.printSentence(Y_all[n], hi_idx2word_all)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
