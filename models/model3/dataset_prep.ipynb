{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data set for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.abspath(os.path.join('.'))\n",
    "module_path = os.path.abspath(os.path.join('..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prabhat_tripathi/dev/Project/NLP/models\n"
     ]
    }
   ],
   "source": [
    "print(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_prep;\n",
    "import utils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en, data_hi = data_prep.loadParallelCorpus('../../data/parallel/IITB.en-hi.en', '../../data/parallel/IITB.en-hi.hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1561841, 1561841)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_rows=data_en.split(\"\\n\")\n",
    "hi_rows=data_hi.split(\"\\n\")\n",
    "(len(en_rows),len(hi_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12565  sentences bad. Removing them...\n"
     ]
    }
   ],
   "source": [
    "# 1. remove sentenses with more than half non-alpha numeric characters\n",
    "to_remove = [i for i, val in enumerate(en_rows) if data_prep.isNonAlpha(val) == True]\n",
    "print(len(to_remove), \" sentences bad. Removing them...\")\n",
    "\n",
    "# process in reverse to avoid recomputing offsets\n",
    "for index in reversed(to_remove):\n",
    "    del en_rows[index]\n",
    "    del hi_rows[index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Shuffle corpora. The sentences in the corpora are not mixed up in the original order\n",
    "en_rows_all, hi_rows_all = data_prep.shuffle(en_rows, hi_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(en_sentences, hi_sentences):\n",
    "    en_sentences = [ line.lower() for line in en_sentences]\n",
    "    \n",
    "    en_vocab_dict = data_prep.buildEngVocab(en_sentences)\n",
    "    hi_vocab_dict = data_prep.buildHinVocab(hi_sentences)\n",
    "\n",
    "    en_vocab = list(map(lambda x: x[0], sorted(en_vocab_dict.items(), key=lambda x: -x[1])))\n",
    "    hi_vocab = list(map(lambda x: x[0], sorted(hi_vocab_dict.items(), key=lambda x: -x[1])))\n",
    "    \n",
    "    # Zipf's law\n",
    "    # https://openreview.net/pdf?id=Bk8N0RLxx - limit vocab to 50k\n",
    "    if (len(en_vocab) > 50000):\n",
    "      en_vocab = en_vocab[:50000]\n",
    "    if (len(hi_vocab) > 50000):\n",
    "      hi_vocab = hi_vocab[:50000]\n",
    "\n",
    "    # Build a Word to Index Dictionary for English\n",
    "    start_idx = 4\n",
    "    en_word2idx = dict([(word, idx + start_idx) for idx, word in enumerate(en_vocab)])\n",
    "    en_word2idx['<pad>'] = 0  # Padding\n",
    "    en_word2idx['<start>'] = 1\n",
    "    en_word2idx['<end>'] = 2  # End of sentence\n",
    "    en_word2idx['<ukn>'] = 3  # Unknown words\n",
    "   \n",
    "\n",
    "    en_vocab.append('<ukn>');\n",
    "    en_vocab.append('<start>');\n",
    "    en_vocab.append('<end>');\n",
    "    en_vocab.append('<pad>');\n",
    "    \n",
    "    # Build reverse Index to Word Dictionary for English using the already created Word to Index Dictionary\n",
    "    en_idx2word = dict([(idx, word) for word, idx in en_word2idx.items()])\n",
    "\n",
    "    # Build a Word to Index Dictionary for Hindi\n",
    "    start_idx = 4\n",
    "    hi_word2idx = dict([(word, idx + start_idx) for idx, word in enumerate(hi_vocab)])\n",
    "    hi_word2idx['<pad>'] = 0  # Padding\n",
    "    hi_word2idx['<start>'] = 1\n",
    "    hi_word2idx['<end>'] = 2  # End of sentence\n",
    "    hi_word2idx['<ukn>'] = 3  # Unknown\n",
    "   \n",
    "\n",
    "    hi_vocab.append('<ukn>');\n",
    "    hi_vocab.append('<start>');\n",
    "    hi_vocab.append('<end>');\n",
    "    hi_vocab.append('<pad>');\n",
    "\n",
    "    \n",
    "    # Build the inverse Index to Word Dictionary for Hindi using the already created Word to Index Dictionary\n",
    "    hi_idx2word = dict([(idx, word) for word, idx in hi_word2idx.items()])\n",
    "\n",
    "    # Encode words in senteces by their index in Vocabulary\n",
    "    x = [[en_word2idx.get(word.strip(',.\" ;:)(][?!'), 3) for word in sentence.split()] for sentence in en_sentences]\n",
    "    y = [[hi_word2idx.get(word.strip(',.\" ;:)(।|][?!'), 3) for word in sentence.split()] for sentence in hi_sentences]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(x)):\n",
    "        n1 = len(x[i])\n",
    "        n2 = len(y[i])\n",
    "        n = n1 if n1 < n2 else n2\n",
    "        if abs(n1 - n2) < 0.3 * n:\n",
    "            if n1 <= max_sent_length and n2 <= max_sent_length:\n",
    "              # ignore single word sentences with only unknown word\n",
    "              if (n1 > 1 or (x[i][0] != 0 and x[i][0] != en_word2idx[''])) and (n2 > 1 or (y[i][0] != 0 and y[i][0] != hi_word2idx[''])):\n",
    "                X.append(x[i])\n",
    "                Y.append(y[i])\n",
    "\n",
    "    return X, Y, en_word2idx, en_idx2word, en_vocab, hi_word2idx, hi_idx2word, hi_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_location = \"out/parallel_trainv3.p\"\n",
    "utils.save_pickle(dataset_save_location, prepare_dataset(en_rows_all, hi_rows_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696695, 696695, 50004, 50004)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us load and check for sanity\n",
    "X_all, Y_all, en_word2idx_all, en_idx2word_all, en_vocab_all, hi_word2idx_all, hi_idx2word_all, hi_vocab_all = utils.load_pickle_dataset(dataset_save_location)\n",
    "len(X_all), len(Y_all), len(en_vocab_all), len(hi_vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jejune \n",
      "\n",
      "अपर्याप्त \n",
      "\n",
      "add \n",
      "\n",
      "जोड़ें \n",
      "\n",
      "yea we are able to make complete his very fingertips \n",
      "\n",
      "हम इस पर क़ादिर हैं कि हम उसकी पोर पोर दुरूस्त करें \n",
      "\n",
      "back up complete \n",
      "\n",
      "बैक अप पूर्ण \n",
      "\n",
      "information education and communication activities iec \n",
      "\n",
      "सूचना शिक्षा और संचार कार्यक्रम आईईसी \n",
      "\n",
      "warning \n",
      "\n",
      "चेतावनीः \n",
      "\n",
      "the task was difficult since indians were utterly unfamiliar with modern political work \n",
      "\n",
      "क्योंकि भारतीय नये राजनीतिक कार्यकलापों से बिल्कुल अपरिचित थे अतः यह कार्य कठिन था \n",
      "\n",
      "that was the level of rhetoric \n",
      "\n",
      "इस स्तर पर चीख-पुकार <ukn> गयी थी \n",
      "\n",
      "mutton egg butter curds and food prepared with oil should be <ukn> \n",
      "\n",
      "माँस अंडे मक्खन दही एवं तेल में पकाये गये खाद्य पदार्थ न दें \n",
      "\n",
      "<ukn> singh \n",
      "\n",
      "<ukn> सिंह \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    data_prep.printSentence(X_all[n], en_idx2word_all)\n",
    "    print(\"\\n\")\n",
    "    data_prep.printSentence(Y_all[n], hi_idx2word_all)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
